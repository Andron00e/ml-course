{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOAh3ItLDlDn"
      },
      "source": [
        "## week10\n",
        "### REINFORCE in PyTorch\n",
        "__This notebook is based on [Practical_RL week06](https://github.com/yandexdataschool/Practical_RL/tree/master/week06_policy_based) materials__\n",
        "\n",
        "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SG9MeTRfDlDp",
        "outputId": "efdb184e-7796-4371-f45b-8dc24c29b955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120893 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.1_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.1_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E1RgO0ZjDlDq",
        "outputId": "63b22eaf-0272-4bb6-ffec-550112fa2f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8gWrTZSDlDq"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q5z9CipDlDq"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j1XVURfYDlDq",
        "outputId": "504ba907-f7af-4337-8d41-2d3f5bf2e942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 4\n",
        "n_actions = 2"
      ],
      "metadata": {
        "id": "1VMW7O02EFch",
        "outputId": "4a062536-a00c-4451-ecd2-650fc20609af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sg3v1SDJDlDq"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  #< YOUR CODE HERE: define a neural network that predicts policy logits >\n",
        "  nn.Linear(n_states, 128),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(128, 64),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(64, n_actions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ePUlrL0DlDr"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after the model we get logits, then our task is to convert logits into probs"
      ],
      "metadata": {
        "id": "8v6fR9pwG9jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "73qMgmfKDlDr"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    #<your code here >\n",
        "    states = torch.FloatTensor(states)\n",
        "    logits = model(states).detach()\n",
        "    probas = torch.softmax(logits, dim=-1).numpy()\n",
        "    return probas #< your code >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C8gb9zBgDlDr"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(\n",
        "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (\n",
        "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1),\n",
        "                   1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcqDAYJpDlDr"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4dF2L9d4DlDr",
        "outputId": "018e765e-511b-467c-b6e7-a46ae8ccd55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def generate_session(t_max=1000):\n",
        "    \"\"\"\n",
        "    play a full session with REINFORCE agent and train at the session end.\n",
        "    returns sequences of states, actions andrewards\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(np.arange(n_actions), p=action_probs) #< your code >\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QCPa31ppDlDs"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFKWGNtpDlDs"
      },
      "source": [
        "### Computing cumulative rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6dQJkUsDDlDs",
        "outputId": "cd85ce36-06a7-4093-bbed-f11c5c032f07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session\n",
        "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    #<your code here >\n",
        "    G = np.zeros(len(rewards))\n",
        "    # it is better to compute cumulative rewards from the end\n",
        "    G[-1] = rewards[-1]\n",
        "    for idx in range(-2, -len(rewards)-1, -1):\n",
        "        G[idx] = rewards[idx] + gamma * G[idx+1]\n",
        "    return G #< array of cumulative rewards >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6gEX2g6SDlDs",
        "outputId": "0ea34b6a-b20f-4136-8c37-203522720ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
        "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4bkD_wfDlDs"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "Following the REINFORCE algorithm, we can define our objective as follows:\n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XvP7LXwrDlDs"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RauzySGDDlDs"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    optimizer.zero_grad() # чтобы быть целее\n",
        "\n",
        "    entropy = -torch.mean(torch.sum(probs * log_probs, dim=1)) #< your code >\n",
        "    loss = -torch.mean(log_probs_for_actions * cumulative_returns) - entropy_coef * entropy #< your code\n",
        "\n",
        "    # Gradient descent step\n",
        "    #< your code >\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OpigLcdDlDs"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": true,
        "id": "QWHnSA0LDlDs",
        "outputId": "2d1771ff-84d9-45c2-a553-a05306e3fedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-ca9df1d36d70>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:21.890\n",
            "mean reward:26.310\n",
            "mean reward:40.570\n",
            "mean reward:61.790\n",
            "mean reward:85.610\n",
            "mean reward:52.730\n",
            "mean reward:44.030\n",
            "mean reward:195.890\n",
            "mean reward:261.360\n",
            "mean reward:325.550\n",
            "mean reward:261.920\n",
            "mean reward:250.080\n",
            "mean reward:248.510\n",
            "mean reward:271.290\n",
            "mean reward:115.520\n",
            "mean reward:272.420\n",
            "mean reward:195.620\n",
            "mean reward:523.700\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session())\n",
        "               for _ in range(100)]  # generate new sessions\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2osiY1AFDlDs"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT3WvWXsDlDs"
      },
      "outputs": [],
      "source": [
        "# record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"),\n",
        "                           directory=\"videos\", force=True)\n",
        "sessions = [generate_session() for _ in range(100)]\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmNfcW-xDlDs"
      },
      "outputs": [],
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "video_path = video_paths[-3]  # You can also try other indices\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # https://stackoverflow.com/a/57378660/1214547\n",
        "    with video_path.open('rb') as fp:\n",
        "        mp4 = fp.read()\n",
        "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "else:\n",
        "    data_url = str(video_path)\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(data_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6SjEnFdDlDt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPf4i0oTDlDt"
      },
      "source": [
        "### Bonus area: solving Acrobot-v1\n",
        "Try to solve more complex environment using Policy gradient method.\n",
        "*Hint: you will need add some imporovements to the original REINFORCE (e.g. Advantage Actor Critic or anything else).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z93cuG-SDlDt"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Acrobot-v1\")\n",
        "env.reset()\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "state_dim = env.reset().shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(state_dim, n_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ7HHEEfDlDt"
      },
      "outputs": [],
      "source": [
        "<Your beautiful code here>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}